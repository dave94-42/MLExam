\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage[]{algorithm}
\usepackage[noend]{algpseudocode}

\author{Carlo Alessi}
\begin{document}

\section{Line Search}
L'algoritmo di line search calcola la grandezza del passo di ottimizzazione che si può fare nella direzione discendente $p_k=-B_k^{-1} \nabla f_k$, dove $B$ è una matrice simmetrica, non-singolare, \textit{positive semi-definite} che approssima la matrice Hessiana, e $\nabla f$ è il gradiente della funzione \footnote{Il pedice $k$ indica l'indice di iterazione dell'algoritmo.}. 
La direzione discendente $p_k$ garantisce che il valore della funzione $f$ può essere diminuito, ed è tale che $p_k^t \nabla f = - \nabla f_k^T B_k^{-1} f_k < 0$, essendo $B$ \textit{positive semi-definite}.

L'obiettivo della line search è trovare un minimizzatore locale della seguente funzione:
\[ \phi (\alpha) = f(x_k + \alpha p_k) \qquad, \alpha > 0 \] 
dove $\alpha$ è lo step length (passo di ottimizzazione), ed output dell'algoritmo di line search. Essendo una ricerca locale, anziché globale, la line search è detta inesatta. 
Ad ogni iterazione $k$ si calcola uno step length $\alpha_k$ e si aggiorna la soluzione candidata $x_k$ in $x_{k+1} = x_k + \alpha_k p_k$.

Per garantire il successo della line search, $\alpha_k$ deve soddisfare una o più delle seguenti condizioni: condizione di Armijo, condizione di Wolfe e condizione forte di Wolfe.

\paragraph{Condizione di Armijo.}
La condizione di Armijo, chiamata anche condizione di decremento sufficiente, è definita dalla seguente disuguaglianza:
\begin{equation}
\begin{aligned}
\phi(\alpha) \leq l(\alpha), \\
l(\alpha) = f(x_k) + c_1 \alpha \nabla f_k^Tp_k \\
\end{aligned}
\label{eq:armijo}
\end{equation}

dove $l(\alpha)$ è una funzione lineare in $\alpha$ con coefficiente angolare $c_1 \nabla f_k^Tp_k < 0$, il quale garantisce che il valore della funzione $f$ diminuisca, e $c_1 \in (0,1)$ è una costante. La condizione di Armijo verrebbe soddisfatta da ogni valore $\alpha$ sufficientemente piccolo, con lo svantaggio che si avrebbe una lenta convergenza dell'algoritmo. D'altra parte un valore di $\alpha$ troppo grande potrebbe andare oltre il punto di minimo cercato. Per questi motivi è opportuno che $\alpha$ soddisfi la seguente condizione.

\paragraph{Condizione di Wolfe.}
La condizione di Wolfe, conosciuta anche come condizione di curvatura, è definita dalla seguente disuguaglianza:

\begin{equation}
\phi '(\alpha) \geq c_2 \phi '(0)
\end{equation}

dove $c_2 \in (c_1, 1)$ è una costante, $\phi'(\alpha_k) = \nabla f(x_k + \alpha_k p_k)^T p_k$, e $\phi'(\alpha) = \nabla f_k^T p_k$. (spiegare per bene, ancora non ho ben capito come funziona)
Per evitare che $\phi'(\alpha) \gg 0$ sia troppo positivo, andrebbero esclusi i punti che vanno troppo oltre un punto stazionario di $\phi$. Per soddisfare questo criterio si può modificare la condizione di curvatura, ottenendo quella che è chiamata la condizione forte di Wolfe, di seguito definita:
\begin{equation}
\mid \phi'(\alpha) \mid \leq c_2 \mid \phi(0) \mid
\end{equation}

\subsection{Algoritmo di Line Search}
L'algoritmo di line search implementato è conosciuto sotto il nome di Backtracking Line Search. L'algoritmo  parte da uno step size $\alpha = \alpha_0$ e lo decrementa finché non trova uno step size $\alpha$ che soddisfa la condizione di Armijo (criterio di stop della line search). Sebbene le condizioni di curvatura assicurano una convergenza più rapida dell'algoritmo di apprendimento, queste possono essere dispensate scegliendo un adeguato valore di $\alpha_0$ (impostato ad 1 per quasi-Newton).
Algoritmo \ref{algo:btls} mostra lo pseudo-codice dell'algoritmo, in cui $c_1 = 10^{-4}$ è costante e $\rho \in (0,1)$ è il fattore di contrazione di $\alpha$. Il valore di $\rho$ sarà deciso empiricamente e potrà variare da iterazione a iterazione.


\begin{algorithm}[H]
\begin{algorithmic}[1]
%\Procedure{MyProcedure}{}
\State $\alpha = 1$
\State $c_1 = 10^{-4}$
\While{\textbf{not} $\phi(\alpha) \leq \phi(0) + c_1 \alpha \phi'(0)$}
\State $\alpha \leftarrow \rho \alpha$
\EndWhile
\State return $\alpha$
%\EndProcedure
\end{algorithmic}
\caption{Backtracking Line Search. La condizione di decremento sufficiente (definita in \eqref{eq:armijo}) è stata riscritta completamente in termini di $\phi$.} \label{algo:btls}
\end{algorithm}

\paragraph{Convergenza della Line Search.} L'algoritmo convergerà in un numero finito di passi, dipendenti dal valore di $\rho$, perché $\alpha$ diventerà sufficientemente piccolo da soddisfare la condizione di Armijo. 



\end{document}